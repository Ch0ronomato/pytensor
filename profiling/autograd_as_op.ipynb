{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "import pytensor.tensor.type as ptt\n",
    "import torch\n",
    "\n",
    "\n",
    "#os.environ[\"OMP_PREFIX\"] = \"/opt/anaconda3/envs/pytensor-dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "from operator import itemgetter\n",
    "from pytensor.compile.builders import construct_nominal_fgraph\n",
    "from pytensor.graph.basic import Apply\n",
    "from pytensor.graph.op import HasInnerGraph, Op\n",
    "from pytensor.link.pytorch.dispatch.basic import pytorch_funcify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutogradOp(Op, HasInnerGraph):\n",
    "    def __init__(self, inputs, outputs, wrt=None):\n",
    "        \n",
    "        self.wrt = []\n",
    "        self.input_types = [i.type for i in inputs]\n",
    "        # todo: It's less that we need\n",
    "        # the outputs, and more that we\n",
    "        # want an fgraph passed in\n",
    "        # todo: Shared variables?\n",
    "        self.fgraph, _, _, _ = construct_nominal_fgraph(\n",
    "            inputs, outputs\n",
    "        )\n",
    "\n",
    "        if wrt is None:\n",
    "            # take all the vars\n",
    "            self.wrt = list(enumerate(inputs))\n",
    "        else:\n",
    "            if not isinstance(wrt, list | tuple):\n",
    "                # take only one var\n",
    "                wrt = [wrt]\n",
    "            if not set(wrt).issubset(set(inputs)):\n",
    "                raise RuntimeError(\n",
    "                    f\"You can differentiate unknown inputs: {wrt}, {inputs}\"\n",
    "                )\n",
    "            self.wrt = [(i, x) for i, x in enumerate(inputs) if x in wrt] \n",
    "\n",
    "    def make_node(self, *inputs):\n",
    "        apply_inputs = [i_t.filter_variable(i) for i, i_t in zip(inputs, self.input_types, strict=True)]\n",
    "        return Apply(self, apply_inputs, [t[1].type() for t in self.wrt])\n",
    "\n",
    "    def perform(self, *args, **kwargs):\n",
    "        raise RuntimeError(\"Should not go to c runtime\")\n",
    "    \n",
    "    def clone(self):\n",
    "        res = copy(self)\n",
    "        res.fgraph = res.fgraph.clone()\n",
    "        return res\n",
    "\n",
    "    @property\n",
    "    def fn(self):\n",
    "        \"\"\"Lazily compile the inner function graph.\"\"\"\n",
    "        if getattr(self, \"_fn\", None) is not None:\n",
    "            return self._fn\n",
    "\n",
    "        self._fn = function(self.inner_inputs, self.inner_outputs, **self.kwargs)\n",
    "        self._fn.trust_input = True\n",
    "\n",
    "        return self._fn\n",
    "\n",
    "    @property\n",
    "    def inner_inputs(self):\n",
    "        return self.fgraph.inputs\n",
    "\n",
    "    @property\n",
    "    def inner_outputs(self):\n",
    "        return self.fgraph.outputs\n",
    "\n",
    "@pytorch_funcify.register(AutogradOp)\n",
    "def autograd(op, node, **kwargs):\n",
    "    inner_fn = pytorch_funcify(op.fgraph, **kwargs, squeeze_output=True)\n",
    "    indicies = list(map(itemgetter(0), op.wrt))\n",
    "\n",
    "    def fn(*inputs):\n",
    "        inputs = [i.requires_grad_(True) for i in inputs]\n",
    "        out = inner_fn(*inputs)\n",
    "        out.backward()\n",
    "        grads = [inputs[i].grad for i in indicies]\n",
    "        if len(grads) == 1:\n",
    "            return grads[0]\n",
    "        else:\n",
    "            return grads\n",
    "\n",
    "    return torch.compiler.disable(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x^2 + 2xy + y^2\n",
    "# for this example, pow is broken\n",
    "from pytensor.scalar.basic import Pow\n",
    "@pytorch_funcify.register(Pow)\n",
    "def pow(op, node, **kwargs):\n",
    "    return torch.pow\n",
    "\n",
    "x = ptt.scalar(\"x\")\n",
    "y = ptt.scalar(\"y\")\n",
    "res = pt.pow(x, 2) + (2 * x * y) + pt.pow(y, 3)\n",
    "autograder = AutogradOp([x, y], [res], [x, y])\n",
    "f = pytensor.function([x, y], autograder(x, y), mode=\"PYTORCH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(10.), array(18.)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "\n",
    "import pymc as pm\n",
    "from pymc import Model\n",
    "from pymc.blocking import DictToArrayBijection, RaveledVars\n",
    "from pymc.initial_point import PointType, make_initial_point_fn\n",
    "from pymc.model.core import modelcontext, join_nonshared_inputs\n",
    "from pymc.model.transform.optimization import freeze_dims_and_data\n",
    "from pymc.vartypes import continuous_types, discrete_types, typefilter\n",
    "from pytensor.graph.basic import Constant, Variable, graph_inputs\n",
    "\n",
    "from pymc.util import (\n",
    "    UNSET,\n",
    "    VarName,\n",
    "    WithMemoization,\n",
    "    _add_future_warning_tag,\n",
    "    _UnsetType,\n",
    "    get_transformed_name,\n",
    "    get_value_vars_from_user_vars,\n",
    "    get_var_name,\n",
    "    treedict,\n",
    "    treelist,\n",
    ")\n",
    "\n",
    "class AutogradModel(Model):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def logp_dlogp_function(self,\n",
    "        grad_vars=None,\n",
    "        tempered=False,\n",
    "        initial_point: PointType | None = None,\n",
    "        ravel_inputs: bool | None = None,\n",
    "        **kwargs,\n",
    "    ): \n",
    "        # this all comes from the base class\n",
    "        if grad_vars is None:\n",
    "            grad_vars = self.continuous_value_vars\n",
    "        else:\n",
    "            grad_vars = get_value_vars_from_user_vars(grad_vars, self)\n",
    "            for i, var in enumerate(grad_vars):\n",
    "                if var.dtype not in continuous_types:\n",
    "                    raise ValueError(f\"Can only compute the gradient of continuous types: {var}\")\n",
    "\n",
    "        if tempered:\n",
    "            costs = [self.varlogp, self.datalogp]\n",
    "        else:\n",
    "            costs = [self.logp()]\n",
    "\n",
    "        input_vars = {i for i in graph_inputs(costs) if not isinstance(i, Constant)}\n",
    "        if initial_point is None:\n",
    "            initial_point = self.initial_point(0)\n",
    "        extra_vars_and_values = {\n",
    "            var: initial_point[var.name]\n",
    "            for var in self.value_vars\n",
    "            if var in input_vars and var not in grad_vars\n",
    "        }\n",
    "        \n",
    "        # new logic\n",
    "        class FnWrapper:\n",
    "            def __init__(\n",
    "                self,\n",
    "                costs,\n",
    "                grad_vars,\n",
    "                extra_vars_and_values=None,\n",
    "                *,\n",
    "                dtype=None,\n",
    "                casting=\"no\",\n",
    "                compute_grads=True,\n",
    "                model=None,\n",
    "                initial_point: PointType | None = None,\n",
    "                ravel_inputs: bool | None = None,\n",
    "                **kwargs,\n",
    "            ):\n",
    "                if extra_vars_and_values is None:\n",
    "                    extra_vars_and_values = {}\n",
    "\n",
    "                names = [arg.name for arg in grad_vars + list(extra_vars_and_values.keys())]\n",
    "                if any(name is None for name in names):\n",
    "                    raise ValueError(\"Arguments must be named.\")\n",
    "                if len(set(names)) != len(names):\n",
    "                    raise ValueError(\"Names of the arguments are not unique.\")\n",
    "\n",
    "                self._grad_vars = grad_vars\n",
    "                self._extra_vars = list(extra_vars_and_values.keys())\n",
    "                self._extra_var_names = {var.name for var in extra_vars_and_values.keys()}\n",
    "\n",
    "                if dtype is None:\n",
    "                    dtype = pytensor.config.floatX\n",
    "                self.dtype = dtype\n",
    "\n",
    "                self._n_costs = len(costs)\n",
    "                if self._n_costs == 0:\n",
    "                    raise ValueError(\"At least one cost is required.\")\n",
    "\n",
    "                cost = costs[0]\n",
    "\n",
    "                self._extra_are_set = False\n",
    "                givens = []\n",
    "                self._extra_vars_shared = {}\n",
    "                for var, value in extra_vars_and_values.items():\n",
    "                    shared = pytensor.shared(value, var.name + \"_shared__\", shape=value.shape)\n",
    "                    self._extra_vars_shared[var.name] = shared\n",
    "                    givens.append((var, shared))\n",
    "\n",
    "                if compute_grads:\n",
    "                    grads = AutogradOp(grad_vars, [cost], grad_vars)\n",
    "                    outputs = [cost, [grads]]\n",
    "                else:\n",
    "                    outputs = [cost]\n",
    "\n",
    "                if ravel_inputs:\n",
    "                    if initial_point is None:\n",
    "                        initial_point = modelcontext(model).initial_point()\n",
    "                    outputs, raveled_grad_vars = join_nonshared_inputs(\n",
    "                        point=initial_point, inputs=grad_vars, outputs=outputs, make_inputs_shared=False\n",
    "                    )\n",
    "                    inputs = [raveled_grad_vars]\n",
    "                else:\n",
    "                    inputs = grad_vars\n",
    "\n",
    "                self._pytensor_function = compile(inputs, outputs, givens=givens, **kwargs)\n",
    "                self._raveled_inputs = ravel_inputs\n",
    "\n",
    "            def __call__(self, grad_vars, *, extra_vars=None):\n",
    "                if extra_vars is not None:\n",
    "                    self.set_extra_values(extra_vars)\n",
    "                elif not self._extra_are_set:\n",
    "                    raise ValueError(\"Extra values are not set.\")\n",
    "\n",
    "                if isinstance(grad_vars, RaveledVars):\n",
    "                    if self._raveled_inputs:\n",
    "                        grad_vars = (grad_vars.data,)\n",
    "                    else:\n",
    "                        grad_vars = DictToArrayBijection.rmap(grad_vars).values()\n",
    "                elif self._raveled_inputs and not isinstance(grad_vars, Sequence):\n",
    "                    grad_vars = (grad_vars,)\n",
    "\n",
    "                return self._pytensor_function(*grad_vars)\n",
    "        return FnWrapper(\n",
    "            costs,\n",
    "            grad_vars,\n",
    "            extra_vars_and_values,\n",
    "            model=self,\n",
    "            initial_point=initial_point,\n",
    "            ravel_inputs=ravel_inputs,\n",
    "            **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Only 10 samples per chain. Reliable r-hat and ESS diagnostics require longer chains for accurate estimate.\n",
      "/home/ischweer/miniconda3/envs/pytensor-dev/lib/python3.11/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Outputs must be pytensor Variable or Out instances. Received AutogradOp of type <class '__main__.AutogradOp'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m AutogradModel() \u001b[38;5;28;01mas\u001b[39;00m m:\n\u001b[1;32m      2\u001b[0m     res \u001b[38;5;241m=\u001b[39m pm\u001b[38;5;241m.\u001b[39mNormal(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblah\u001b[39m\u001b[38;5;124m\"\u001b[39m, mu\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, sigma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mpm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/pymc/pymc/sampling/mcmc.py:804\u001b[0m, in \u001b[0;36msample\u001b[0;34m(draws, tune, chains, cores, random_seed, progressbar, progressbar_theme, step, var_names, nuts_sampler, initvals, init, jitter_max_retries, n_init, trace, discard_tuned_samples, compute_convergence_checks, keep_warning_stat, return_inferencedata, idata_kwargs, nuts_sampler_kwargs, callback, mp_ctx, blas_cores, model, compile_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    802\u001b[0m         [kwargs\u001b[38;5;241m.\u001b[39msetdefault(k, v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m nuts_kwargs\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m    803\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m joined_blas_limiter():\n\u001b[0;32m--> 804\u001b[0m         initial_points, step \u001b[38;5;241m=\u001b[39m \u001b[43minit_nuts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m            \u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m            \u001b[49m\u001b[43mjitter_max_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjitter_max_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtune\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m            \u001b[49m\u001b[43minitvals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitvals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompile_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;66;03m# Get initial points\u001b[39;00m\n\u001b[1;32m    819\u001b[0m     ipfns \u001b[38;5;241m=\u001b[39m make_initial_point_fns_per_chain(\n\u001b[1;32m    820\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    821\u001b[0m         overrides\u001b[38;5;241m=\u001b[39minitvals,\n\u001b[1;32m    822\u001b[0m         jitter_rvs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mset\u001b[39m(),\n\u001b[1;32m    823\u001b[0m         chains\u001b[38;5;241m=\u001b[39mchains,\n\u001b[1;32m    824\u001b[0m     )\n",
      "File \u001b[0;32m~/dev/pymc/pymc/sampling/mcmc.py:1502\u001b[0m, in \u001b[0;36minit_nuts\u001b[0;34m(init, chains, n_init, model, random_seed, progressbar, jitter_max_retries, tune, initvals, compile_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madvi\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m init:\n\u001b[1;32m   1497\u001b[0m     cb \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1498\u001b[0m         pm\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mCheckParametersConvergence(tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m, diff\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabsolute\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1499\u001b[0m         pm\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mCheckParametersConvergence(tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m, diff\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelative\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1500\u001b[0m     ]\n\u001b[0;32m-> 1502\u001b[0m logp_dlogp_func \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogp_dlogp_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mravel_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcompile_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1503\u001b[0m logp_dlogp_func\u001b[38;5;241m.\u001b[39mtrust_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m initial_points \u001b[38;5;241m=\u001b[39m _init_jitter(\n\u001b[1;32m   1505\u001b[0m     model,\n\u001b[1;32m   1506\u001b[0m     initvals,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1510\u001b[0m     logp_dlogp_func\u001b[38;5;241m=\u001b[39mlogp_dlogp_func,\n\u001b[1;32m   1511\u001b[0m )\n",
      "Cell \u001b[0;32mIn[41], line 140\u001b[0m, in \u001b[0;36mAutogradModel.logp_dlogp_function\u001b[0;34m(self, grad_vars, tempered, initial_point, ravel_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             grad_vars \u001b[38;5;241m=\u001b[39m (grad_vars,)\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pytensor_function(\u001b[38;5;241m*\u001b[39mgrad_vars)\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFnWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcosts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_vars_and_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_point\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_point\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mravel_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mravel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[41], line 115\u001b[0m, in \u001b[0;36mAutogradModel.logp_dlogp_function.<locals>.FnWrapper.__init__\u001b[0;34m(self, costs, grad_vars, extra_vars_and_values, dtype, casting, compute_grads, model, initial_point, ravel_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m initial_point \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m         initial_point \u001b[38;5;241m=\u001b[39m modelcontext(model)\u001b[38;5;241m.\u001b[39minitial_point()\n\u001b[0;32m--> 115\u001b[0m     outputs, raveled_grad_vars \u001b[38;5;241m=\u001b[39m \u001b[43mjoin_nonshared_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_point\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmake_inputs_shared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m [raveled_grad_vars]\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/dev/pymc/pymc/pytensorf.py:621\u001b[0m, in \u001b[0;36mjoin_nonshared_inputs\u001b[0;34m(point, outputs, inputs, shared_inputs, make_inputs_shared)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shared_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    619\u001b[0m     replace\u001b[38;5;241m.\u001b[39mupdate(shared_inputs)\n\u001b[0;32m--> 621\u001b[0m new_outputs \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpytensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrebuild_strict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_outputs, joined_inputs\n",
      "File \u001b[0;32m~/dev/pymc/pymc/pytensorf.py:622\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shared_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    619\u001b[0m     replace\u001b[38;5;241m.\u001b[39mupdate(shared_inputs)\n\u001b[1;32m    621\u001b[0m new_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 622\u001b[0m     \u001b[43mpytensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrebuild_strict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs\n\u001b[1;32m    623\u001b[0m ]\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_outputs, joined_inputs\n",
      "File \u001b[0;32m~/miniconda3/envs/pytensor-dev/lib/python3.11/site-packages/pytensor/graph/replace.py:82\u001b[0m, in \u001b[0;36mclone_replace\u001b[0;34m(output, replace, **rebuild_kwds)\u001b[0m\n\u001b[1;32m     80\u001b[0m tmp_replace \u001b[38;5;241m=\u001b[39m [(x, x\u001b[38;5;241m.\u001b[39mtype()) \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m items]\n\u001b[1;32m     81\u001b[0m new_replace \u001b[38;5;241m=\u001b[39m [(x, y) \u001b[38;5;28;01mfor\u001b[39;00m ((_, x), (_, y)) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tmp_replace, items, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)]\n\u001b[0;32m---> 82\u001b[0m _, _outs, _ \u001b[38;5;241m=\u001b[39m \u001b[43mrebuild_collect_shared\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmp_replace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrebuild_kwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# TODO Explain why we call it twice ?!\u001b[39;00m\n\u001b[1;32m     85\u001b[0m _, outs, _ \u001b[38;5;241m=\u001b[39m rebuild_collect_shared(_outs, [], new_replace, [], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrebuild_kwds)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytensor-dev/lib/python3.11/site-packages/pytensor/compile/function/pfunc.py:319\u001b[0m, in \u001b[0;36mrebuild_collect_shared\u001b[0;34m(outputs, inputs, replace, updates, rebuild_strict, copy_inputs_over, no_default_updates, clone_inner_graphs)\u001b[0m\n\u001b[1;32m    317\u001b[0m             cloned_outputs\u001b[38;5;241m.\u001b[39mappend(Out(cloned_v, borrow\u001b[38;5;241m=\u001b[39mv\u001b[38;5;241m.\u001b[39mborrow))\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    320\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputs must be pytensor Variable or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOut instances. Received \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(v) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(v))\n\u001b[1;32m    322\u001b[0m             )\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;66;03m# computed_list.append(cloned_v)\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, Variable):\n",
      "\u001b[0;31mTypeError\u001b[0m: Outputs must be pytensor Variable or Out instances. Received AutogradOp of type <class '__main__.AutogradOp'>"
     ]
    }
   ],
   "source": [
    "with AutogradModel() as m:\n",
    "    res = pm.Normal(\"blah\", mu=0, sigma=1)\n",
    "    pm.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytensor-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
